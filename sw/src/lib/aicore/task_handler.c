// (C) Copyright Axelera AI 2024
// All Rights Reserved
// *** Axelera AI Confidential ***

// THIS FILE IS AUTOMATICALLY GENERATED. DO NOT MODIFY.
// Use the "blueprint-engine" to regenerate this file.

/* clang-format off */

//==================================================
// INCLUDES
//==================================================

#include "task_handler.h"

#include "sdk_adapter.h"

#include "module.h"
#include "token_manager.h"
#include "datapath.h"
#include "mempool.h"
#include "dma_hp.h"
#include "dma_lp.h"

//==================================================
// MACROS
//==================================================

//==================================================
// DEFINITIONS
//==================================================

//==================================================
// TYPES
//==================================================

//==================================================
// DATA
//==================================================

#if LOG_LEVEL <= LOG_LEVEL_ERROR
static const char* task_type_strings[] = {
  [TASK_HANDLER_TASK_TYPE_COMMAND] = "command",
  [TASK_HANDLER_TASK_TYPE_INSTRUCTION] = "instruction",
  [TASK_HANDLER_TASK_TYPE_TOKEN] = "token",
  [TASK_HANDLER_TASK_TYPE_POLLING] = "polling",
  [TASK_HANDLER_TASK_TYPE_START_DMA] = "start_dma",
  [TASK_HANDLER_TASK_TYPE_POLL_DMA] = "poll_dma",
  [TASK_HANDLER_TASK_TYPE_C2C_SYNC] = "c2c_sync",
};
#endif

static uint32_t task_idx = 0;

//==================================================
// LOCAL FUNCTION DECLARATIONS
//==================================================

static task_handler_status_t handle_packed_task(const task_handler_task_t* task);
static task_handler_status_t handle_indirect_task(const task_handler_task_t* task);
static task_handler_status_t handle_packed_tasks(uintptr_t packed_tasks_ptr, size_t packed_tasks_size);

//==================================================
// LOCAL FUNCTION DEFINITIONS
//==================================================

static inline task_handler_status_t handle_packed_task(const task_handler_task_t* const task) {
  LOG_INFO("Handling task %d (%s)", task_idx, task_type_strings[task->header.type]);
  int status = 0;

  switch (task->header.type) {
    case TASK_HANDLER_TASK_TYPE_COMMAND:
      LOG_DEBUG("Handling packed command task");
      status = (int) module_load_command(
        (module_id_t) task->command.payload.packed.target,
        (module_command_t*) task->command.payload.packed.payload
      );
      break;

    case TASK_HANDLER_TASK_TYPE_INSTRUCTION:
      LOG_DEBUG("Handling packed instruction task");
      status = (int) module_load_instructions(
        (module_id_t) task->instruction.payload.packed.target,
        (uint64_t*) task->instruction.payload.packed.payload,
        task->instruction.payload.packed.count,
        task->instruction.payload.packed.offset
      );
      break;

    case TASK_HANDLER_TASK_TYPE_TOKEN:
      if (task->token.payload.mode == TOKEN_MANAGER_TOKEN_MODE_PRODUCE) {
        LOG_DEBUG("Handling 'produce' token task");
        status = (int) token_manager_produce_multiple_tokens(
          task->token.payload.targets
        );
      } else {
        LOG_DEBUG("Handling 'consume' token task");
        status = (int) token_manager_consume_multiple_tokens(
          task->token.payload.targets
        );
      }
      break;

    case TASK_HANDLER_TASK_TYPE_POLLING:
      LOG_DEBUG("Handling polling task");
      for (module_id_t i = 0; i < MODULE_COUNT; i++) {
        if (task->polling.payload.targets[i]) {
          status = (int) module_poll_idle(i);
          if (status) break;
        }
      }
      break;

    case TASK_HANDLER_TASK_TYPE_START_DMA:
      LOG_DEBUG("Handling packed start DMA task");
      uint32_t transfer_count = task->start_dma.payload.packed.count;
      const task_handler_memory_transfer_t* transfers = (task_handler_memory_transfer_t*) task->start_dma.payload.packed.transfers;

      ASSERT(transfer_count > 0);

      if (transfer_count == 1) {
        status = (int) dma_hp_start(
          (uintptr_t) mempool_get_addr(&transfers->src),
          (uintptr_t) mempool_get_addr(&transfers->dst),
          transfers->size,
          task->start_dma.payload.packed.ch_idx
        );
        break;
      } else {
        // Append all memory transfers to the linked list
        for (uint32_t i = 0; i < transfer_count; i++) {
          const task_handler_memory_transfer_t* transfer = &transfers[i];
          status = (int) dma_hp_ll_append(
            (uintptr_t) mempool_get_addr(&transfer->src),
            (uintptr_t) mempool_get_addr(&transfer->dst),
            transfer->size,
            task->start_dma.payload.packed.ch_idx
          );
          if (status) break;
        }

        if (!status) {
          // Start the linked list transfer
          status = (int) dma_hp_ll_start(task->start_dma.payload.packed.ch_idx);
        }
      }
      break;

    case TASK_HANDLER_TASK_TYPE_POLL_DMA:
      LOG_DEBUG("Handling poll DMA task");
      status = (int) dma_hp_poll(task->poll_dma.payload.ch_idx);
      break;


    default:
      LOG_ERROR("Task %d has unknown type: %d", task_idx, task->header.type);
      return TASK_HANDLER_STATUS_ERROR;
  }

  if (status) {
    LOG_ERROR("Task %d failed (type: %s, status: %d)", task_idx, task_type_strings[task->header.type], status);
    return TASK_HANDLER_STATUS_ERROR;
  }

  LOG_DEBUG("Task handled");
  return TASK_HANDLER_STATUS_OK;
}

static inline task_handler_status_t handle_indirect_task(const task_handler_task_t* const task) {
  LOG_INFO("Handling task %d (%s)", task_idx, task_type_strings[task->header.type]);
  int status = 0;

  switch (task->header.type) {
    case TASK_HANDLER_TASK_TYPE_COMMAND:
      LOG_DEBUG("Handling indirect command task");
      status = (int) module_load_command(
        (module_id_t) task->command.payload.indirect.target,
        (module_command_t*) task->command.payload.indirect.payload
      );
      break;

    case TASK_HANDLER_TASK_TYPE_INSTRUCTION:
      LOG_DEBUG("Handling indirect instruction task");
      status = (int) module_load_instructions(
        (module_id_t) task->instruction.payload.indirect.target,
        (uint64_t*) task->instruction.payload.indirect.payload,
        task->instruction.payload.indirect.count,
        task->instruction.payload.indirect.offset
      );
      break;

    case TASK_HANDLER_TASK_TYPE_TOKEN:
      if (task->token.payload.mode == TOKEN_MANAGER_TOKEN_MODE_PRODUCE) {
        LOG_DEBUG("Handling 'produce' token task");
        status = (int) token_manager_produce_multiple_tokens(
          task->token.payload.targets
        );
      } else {
        LOG_DEBUG("Handling 'consume' token task");
        status = (int) token_manager_consume_multiple_tokens(
          task->token.payload.targets
        );
      }
      break;

    case TASK_HANDLER_TASK_TYPE_POLLING:
      LOG_DEBUG("Handling polling task");
      for (module_id_t i = 0; i < MODULE_COUNT; i++) {
        if (task->polling.payload.targets[i]) {
          status = (int) module_poll_idle(i);
          if (status) break;
        }
      }
      break;

    case TASK_HANDLER_TASK_TYPE_START_DMA:
      LOG_DEBUG("Handling indirect start DMA task");
      uint32_t transfer_count = task->start_dma.payload.indirect.count;
      const task_handler_memory_transfer_t* transfers = (task_handler_memory_transfer_t*) task->start_dma.payload.indirect.transfers;

      ASSERT(transfer_count > 0);

      if (transfer_count == 1) {
        status = (int) dma_hp_start(
          (uintptr_t) mempool_get_addr(&transfers->src),
          (uintptr_t) mempool_get_addr(&transfers->dst),
          transfers->size,
          task->start_dma.payload.indirect.ch_idx
        );
        break;
      } else {
        // Append all memory transfers to the linked list
        for (uint32_t i = 0; i < transfer_count; i++) {
          const task_handler_memory_transfer_t* transfer = &transfers[i];
          status = (int) dma_hp_ll_append(
            (uintptr_t) mempool_get_addr(&transfer->src),
            (uintptr_t) mempool_get_addr(&transfer->dst),
            transfer->size,
            task->start_dma.payload.indirect.ch_idx
          );
          if (status) break;
        }

        if (!status) {
          // Start the linked list transfer
          status = (int) dma_hp_ll_start(task->start_dma.payload.indirect.ch_idx);
        }
      }
      break;

    case TASK_HANDLER_TASK_TYPE_POLL_DMA:
      LOG_DEBUG("Handling poll DMA task");
      status = (int) dma_hp_poll(task->poll_dma.payload.ch_idx);
      break;


    default:
      LOG_ERROR("Task %d has unknown type: %d", task_idx, task->header.type);
      return TASK_HANDLER_STATUS_ERROR;
  }

  if (status) {
    LOG_ERROR("Task %d failed (type: %s, status: %d)", task_idx, task_type_strings[task->header.type], status);
    return TASK_HANDLER_STATUS_ERROR;
  }

  LOG_DEBUG("Task handled");
  return TASK_HANDLER_STATUS_OK;
}

static task_handler_status_t handle_packed_tasks(uintptr_t packed_tasks_ptr, size_t packed_tasks_size) {
  uint32_t offset = 0;
  while (offset < packed_tasks_size) {
    task_handler_task_t* task_ptr = (task_handler_task_t*) (packed_tasks_ptr + offset);
    task_handler_status_t status = handle_packed_task(task_ptr);
    if (status != TASK_HANDLER_STATUS_OK) {
      LOG_ERROR("Task %d failed", task_idx);
      return TASK_HANDLER_STATUS_ERROR;
    }
    offset += task_ptr->header.size;
    task_idx++;
  }
  if (offset != packed_tasks_size) {
    LOG_ERROR("Packed tasks size mismatch: %d != %d", offset, packed_tasks_size);
    return TASK_HANDLER_STATUS_ERROR;
  }
  return TASK_HANDLER_STATUS_OK;
}

//==================================================
// GLOBAL FUNCTION DEFINITIONS
//==================================================

task_handler_status_t task_handler_init(void) {
  LOG_DEBUG("Initializing task handler");

  module_init();
  datapath_init();
  token_manager_init();
  dma_hp_init();
  dma_lp_init();

  return TASK_HANDLER_STATUS_OK;
}

task_handler_status_t task_handler_run_tasks(const task_handler_task_t* const tasks[], const uint32_t count) {
  LOG_INFO("Running %d tasks", count);
  task_idx = 0;
  while (task_idx < count) {
    task_handler_status_t status = handle_indirect_task(tasks[task_idx]);
    if (status != TASK_HANDLER_STATUS_OK) {
      LOG_ERROR("Task %d failed", task_idx);
      return TASK_HANDLER_STATUS_ERROR;
    }
    task_idx++;
  }
  LOG_INFO("All tasks executed successfully");
  return TASK_HANDLER_STATUS_OK;
}

task_handler_status_t task_handler_run_packed_tasks(uintptr_t packed_tasks_ptr, size_t packed_tasks_size) {
  LOG_INFO("Running packed tasks (size: %d)", packed_tasks_size);
  task_idx = 0;
  return handle_packed_tasks(packed_tasks_ptr, packed_tasks_size);
}

task_handler_status_t task_handler_stream_packed_tasks(const task_handler_packed_tasks_stream_t* stream, uintptr_t buffer, size_t buffer_size) {
  LOG_INFO("Streaming packed tasks (size: %d, chunk count: %d, max chunk size: %d)", stream->packed_tasks_size, stream->chunk_count, stream->chunk_size_limit);
  ASSERT(stream->chunk_size_limit % 64 == 0);
  ASSERT(stream->packed_tasks_ptr % 64 == 0);

  // Allocate the stream buffer on the stack if no external buffer is provided.
  // The next few lines are ugly, because there is no nice way to conditionally allocate
  // a buffer on the stack. The lifetime would be limited to the if-scope.
  bool use_stack_buffer = buffer == 0;
  size_t stack_buffer_size = use_stack_buffer ? 2 * stream->chunk_size_limit : 1;
  uint8_t __attribute__((aligned(64))) stack_buffer[stack_buffer_size];
  if (use_stack_buffer) {
    buffer = (uintptr_t) stack_buffer;
    buffer_size = 2 * stream->chunk_size_limit;
  } else if (buffer_size != 2 * stream->chunk_size_limit) {
    LOG_ERROR("Buffer size mismatch: %d != %d", buffer_size, 2 * stream->chunk_size_limit);
    return TASK_HANDLER_STATUS_ERROR;
  }

  uintptr_t buffers[2] = {
    buffer,
    buffer + stream->chunk_size_limit
  };
  uintptr_t next_chunk_ptr = stream->packed_tasks_ptr;

  task_idx = 0;
  for (int chunk_idx = 0; chunk_idx <= stream->chunk_count; chunk_idx++) {

    // Initiate load of next chunk (except in last iteration)
    if (chunk_idx < stream->chunk_count) {
      uintptr_t load_buffer = buffers[chunk_idx % 2];
      LOG_DEBUG("Loading tasklist chunk %d to %p", chunk_idx, (void*) load_buffer);
      dma_lp_status_t status = dma_lp_start(next_chunk_ptr, load_buffer, stream->chunk_sizes[chunk_idx]);
      if (status != DMA_LP_STATUS_OK) {
        LOG_ERROR("Failed to start DMA transfer for tasklist chunk %d", chunk_idx);
        return TASK_HANDLER_STATUS_ERROR;
      }
      next_chunk_ptr += stream->chunk_sizes[chunk_idx];
    }

    // Execute tasklist chunk (except in first iteration)
    if (chunk_idx > 0) {
      uintptr_t exec_buffer = buffers[(chunk_idx - 1) % 2];
      task_handler_status_t status = handle_packed_tasks(exec_buffer, stream->chunk_sizes[chunk_idx - 1]);
      if (status != TASK_HANDLER_STATUS_OK) {
        LOG_ERROR("Chunk %d failed", task_idx, chunk_idx - 1);
        return TASK_HANDLER_STATUS_ERROR;
      }
    }

    // Make sure next chunk is loaded (except in last iteration)
    if (chunk_idx < stream->chunk_count) {
      LOG_DEBUG("Polling DMA transfer for tasklist chunk %d", chunk_idx);
      dma_lp_status_t status = dma_lp_poll();
      if (status != DMA_LP_STATUS_OK) {
        LOG_ERROR("Failed to poll DMA transfer for tasklist chunk %d", chunk_idx);
        return TASK_HANDLER_STATUS_ERROR;
      }
    }
  }

  LOG_INFO("Task stream executed successfully");
  return TASK_HANDLER_STATUS_OK;
}
