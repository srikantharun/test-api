/*======================================================================*/
/* Europa firmware verification linker script                           */
/*                                                                      */
/*                                                                      */
/* This file was autogenerated using ld_generator.py and verifsdk.yaml  */
/* File generated at {{ current_time }}.                              */
/*======================================================================*/

#include "memorymap.h"
#include "stack.h"

/*----------------------------------------------------------------------*/
/* Setup                                                                */
/*----------------------------------------------------------------------*/

OUTPUT_ARCH( "riscv" )

{% for aicore in aicores %}
/* Entry point: used by Spike */
#ifdef SYSTEM_AICORE
ENTRY(aicore{{ aicore }}__start)
#else
ENTRY(_start)
#endif

{% endfor %}
/*----------------------------------------------------------------------*/
/* UVM SW IPC memory allocation */
/*----------------------------------------------------------------------*/

/* Sizes: 64 bytes per HART, rounded up to next power of two */
#define UVM_SW_IPC_SIZE        (0x40)
#define APU_UVM_SW_IPC_SIZE    (UVM_SW_IPC_SIZE * 6)     /* 6 HARTs */
#define AICORE_UVM_SW_IPC_SIZE (UVM_SW_IPC_SIZE * {{aicores|length}})     /* 1 HART per instance */
#define PVE_UVM_SW_IPC_SIZE    (UVM_SW_IPC_SIZE * 8 * {{pves|length}}) /* 8 HARTs per instance */

/* Addresses: placed at the end of the sys_spm.
 * These are hard-coded in UVM code.
 * Memory Map:
 *
 * SYS_SPM_BASE                            SYS_SPM_BASE + SYS_SPM_SIZE
 *      +-----------------------------------------------+
 *      |        | pve_1 | pve_0 | aicore 7-0 | apu 5-0 |
 *      +-----------------------------------------------+
 */
#define UVM_SW_IPC_BASE        (SYS_SPM_BASE + SYS_SPM_SIZE - UVM_SW_IPC_SIZE)

/*----------------------------------------------------------------------*/
/* Sections                                                             */
/*----------------------------------------------------------------------*/

/*
 * WARNING(europa!248):
 *
 * Consider the following snippet:
 *     .text ALIGN(16) : { *(.text) }
 *
 * When using the GNU linker (ld.bfd), the section's start address will be
 * 16-byte aligned. This is expected behavior.
 *
 * When using the LLVM linker (ld.lld) however, this will not work. As a
 * workaround, use the following snippet:
 *     . = ALIGN(16);
 *     .text : { *(.text) }
 */


MEMORY
{
  /* top */
  sys_spm     : ORIGIN = SYS_SPM_BASE, LENGTH = SYS_SPM_SIZE - APU_UVM_SW_IPC_SIZE - AICORE_UVM_SW_IPC_SIZE - PVE_UVM_SW_IPC_SIZE
  dram        : ORIGIN = DDR_1_BASE,   LENGTH = DDR_1_SIZE
  l2          : ORIGIN = L2_BASE,   LENGTH = L2_SIZE

  /* AI cores */
  {% for aicore in aicores %}
  aicore{{ aicore }}_l1  : ORIGIN = AICORE_{{ aicore }}_L1_BASE,  LENGTH = AICORE_{{ aicore }}_L1_SIZE
  aicore{{ aicore }}_spm : ORIGIN = AICORE_{{ aicore }}_SPM_BASE, LENGTH = AICORE_{{ aicore }}_SPM_SIZE
  {% endfor %}

  /* PVEs */
  {% for pve in pves %}
  pve{{ pve }}_spm    : ORIGIN = PVE_{{ pve }}_SPM_BASE,  LENGTH = PVE_{{ pve }}_SPM_SIZE
  {% for l1 in range(4) %}
  pve{{ pve }}_l1_{{ l1 }}   : ORIGIN = PVE_{{ pve }}_L1_{{ l1 }}_BASE, LENGTH = PVE_{{ pve }}_L1_{{ l1 }}_SIZE
  {% endfor %}
  {% endfor %}
}

SECTIONS
{
  /* === */
  /* APU */
  /* === */
#if PROCESSOR_APU
  /* APU boot code */
  .text.init : ALIGN(64) {
    *(.text.init)
  } > sys_spm
  ASSERT(
    ADDR(.text.init) == ORIGIN(sys_spm),
    "Error: APU boot code not at start of Sys-SPM"
  )

  /* magic memory for FESVR communication */
  .tohost : ALIGN(0x1000) {
    *(.tohost)
    *(.*.tohost)
    . = ALIGN(0x1000); /* ensure proper alignment of next section */
  } > sys_spm

  /* APU code */
  .text : ALIGN(64) {
    *(.text .text.*)
  } > sys_spm

  /* APU data/BSS*/
  .rodata : ALIGN(64) {
    *(.rodata  .rodata.*)
  } > sys_spm
  .data : ALIGN(64) {
    *(.data .data.*)
  } > sys_spm
  .sdata : ALIGN(64) {
    __global_pointer$ = . + 0x800;
    *(.srodata .srodata.*)
    *(.sdata .sdata.*)
    *(.gnu.linkonce.s.*)
  } > sys_spm
  /* BSS: will be zero-initialized by runtime to ensure it is zero even when
   *      side-loading binary using GDB */
  .sbss : ALIGN(64) {
    _bss_begin = .;
    *(.sbss .sbss.*)
    *(.scommon .scommon.*)
    *(.gnu.linkonce.sb.*)
  } > sys_spm
  .bss : ALIGN(64) {
    *(.bss)
    . = ALIGN(64); /* ensure _bss_end is 64-byte aligned */
    _bss_end = .;
  } > sys_spm
#endif

#if SYSTEM_TOP
  /* For the top system, put all .tdata sections consecutively into Sys-SPM. */
  /* thread-local data/BSS for all processors */
  /* 64-byte aligned as this is the same alignment that will be used at runtime */
  . = ALIGN(64);
  _thread_pointer = .; /* required to calculate thread-pointer offset on all processors */

  /* .tdata and .tbss are merged to ensure they are both allocated properly.
   * LLD has some weird behavior where it overlaps them when they are put into
   * separate output sections. */

  /* TLS for APU */
  .tdata : ALIGN(64) {
    _tdata_begin = .;
    *(.tdata .tdata.*)
    . = ALIGN(64); /* required to ensure proper alignment at runtime as .tbss will be placed immediately after .tdata at runtime */
    _tdata_end = .;
    _tbss_begin = .;
    *(.tbss .tbss.*)
    . = ALIGN(64);
    _tbss_end = .;
    _tls_alloc_size = _tbss_end - _tdata_begin;

  /* AICORE .tdata/.tbss placed here contiguously */
  {% for aicore in aicores %}
#if PROCESSOR_AICORE{{ aicore }}
    /* TLS for AICORE {{ aicore }} */
    aicore{{ aicore }}__tdata_begin = .;
    *(.aicore{{ aicore }}.tdata .aicore{{ aicore }}.tdata.*)
    . = ALIGN(64);
    aicore{{ aicore }}__tdata_end = .;
    aicore{{ aicore }}__tbss_begin = .;
    *(.aicore{{ aicore }}.tbss .aicore{{ aicore }}.tbss.*)
    . = ALIGN(64);
    aicore{{ aicore }}__tbss_end = .;
    aicore{{ aicore }}__tls_alloc_size = aicore{{ aicore }}__tbss_end - aicore{{ aicore }}__tdata_begin;
#endif

  {% endfor %}
  /* PVE .tdata/.tbss placed here contiguously */
  {% for pve in pves %}
#if PROCESSOR_PVE{{ pve }}
    /* TLS for PVE {{ pve }} */
    pve{{ pve }}__tdata_begin = .;
    *(.pve{{ pve }}.tdata .pve{{ pve }}.tdata.*)
    . = ALIGN(64);
    pve{{ pve }}__tdata_end = .;
    pve{{ pve }}__tbss_begin = .;
    *(.pve{{ pve }}.tbss .pve{{ pve }}.tbss.*)
    . = ALIGN(64);
    pve{{ pve }}__tbss_end = .;
    pve{{ pve }}__tls_alloc_size = pve{{ pve }}__tbss_end - pve{{ pve }}__tdata_begin;
#endif

{% endfor %}
  } > sys_spm
#endif

#if PROCESSOR_APU
  .runtime.tls : ALIGN(64) {
    _tls_begin = .;
    . = . + 6 * _tls_alloc_size;
    _tls_end = .;
  } > sys_spm
  .runtime.stack : ALIGN(64) {
    _stack_begin = .;
    . = . + 6 * APU_STACK_SIZE;
    _stack_end = .;
  } > sys_spm

  .ddr : ALIGN(64) {
    *(.ddr .ddr.*)
  } > dram

  /* magic memory for UVM SW IPC communication */
  PROVIDE(_uvm_sw_ipc_mem = UVM_SW_IPC_BASE);
#endif

/* Always enable explicit allocation to system SPM from any core */
.sys_spm : ALIGN(64) {
  *(*.sys_spm *.sys_spm.*)
} > sys_spm

/* Always enable explicit allocation to L2 memory from any core */
.l2 : ALIGN(64) {
  *(*.l2 *.l2.*)
} > l2

{% for aicore in aicores %}
  /* ======= */
  /* AICORE{{ aicore }} */
  /* ======= */
#if PROCESSOR_AICORE{{ aicore }}
  .aicore{{ aicore }}.text : ALIGN(64) {
    KEEP(*(.aicore{{ aicore }}.text.init))
    *(.aicore{{ aicore }}.text .aicore{{ aicore }}.text.*)
  } > aicore{{ aicore }}_spm
  ASSERT(
    ADDR(.aicore{{ aicore }}.text) == ORIGIN(aicore{{ aicore }}_spm),
    "Error: AICORE{{ aicore }} boot code not at start of AICORE{{ aicore }} SPM"
  )
  .aicore{{ aicore }}.data : ALIGN(64) {
    *(.aicore{{ aicore }}.operation_data_type.*)
    *(.aicore{{ aicore }}.rodata .aicore{{ aicore }}.rodata.*)
    *(.aicore{{ aicore }}.data .aicore{{ aicore }}.data.*)
  } > aicore{{ aicore }}_spm
  .aicore{{ aicore }}.sdata : ALIGN(64) {
    aicore{{ aicore }}___global_pointer$ = . + 0x800;
    *(.aicore{{ aicore }}.srodata .aicore{{ aicore }}.srodata.*)
    *(.aicore{{ aicore }}.sdata .aicore{{ aicore }}.sdata.*)
  } > aicore{{ aicore }}_spm
  .aicore{{ aicore }}.sbss : ALIGN(64) {
    aicore{{ aicore }}__bss_begin = .;
    *(.aicore{{ aicore }}.sbss .aicore{{ aicore }}.sbss.*)
  } > aicore{{ aicore }}_spm
  .aicore{{ aicore }}.bss : ALIGN(64) {
    *(.aicore{{ aicore }}.bss .aicore{{ aicore }}.bss.*)
    aicore{{ aicore }}__bss_end = .;
  } > aicore{{ aicore }}_spm

  /* Enable explicit allocation to AI Core {{ aicore }} internal SPM from any core */
  .aicore{{ aicore }}.spm : ALIGN(64) {
    *(*.aicore{{ aicore }}.spm *.aicore{{ aicore }}.spm.*)
  } > aicore{{ aicore }}_spm

#ifndef SYSTEM_TOP
  /* TLS for AICORE {{ aicore }} (standalone) */
  .aicore{{ aicore }}.tdata : ALIGN(64) {
    aicore{{ aicore }}__tdata_begin = .;
    *(.aicore{{ aicore }}.tdata .aicore{{ aicore }}.tdata.*)
    . = ALIGN(64);
    aicore{{ aicore }}__tdata_end = .;
    aicore{{ aicore }}__tbss_begin = .;
    *(.aicore{{ aicore }}.tbss .aicore{{ aicore }}.tbss.*)
    . = ALIGN(64);
    aicore{{ aicore }}__tbss_end = .;
  } > aicore{{ aicore }}_spm
  aicore{{ aicore }}__tls_alloc_size = aicore{{ aicore }}__tbss_end - aicore{{ aicore }}__tdata_begin;

  /* magic memory for FESVR communication */
  .tohost : ALIGN(0x1000) {
    *(.tohost)
    *(.*.tohost)
    . = ALIGN(0x1000); /* ensure proper alignment of next section */
  } > aicore{{ aicore }}_spm
#endif

  .aicore{{ aicore }}.runtime.tls : ALIGN(64) {
    aicore{{ aicore }}__tls_begin = .;
    . = . + aicore{{ aicore }}__tls_alloc_size;
    aicore{{ aicore }}__tls_end = .;
  } > aicore{{ aicore }}_spm
  .aicore{{ aicore }}.runtime.stack : ALIGN(64) {
    aicore{{ aicore }}__stack_begin = .;
    . = . + AICORE_STACK_SIZE;
    aicore{{ aicore }}__stack_end = .;
  } > aicore{{ aicore }}_spm

  .aicore{{ aicore }}.l1 : ALIGN(64) {
    *(.aicore{{ aicore }}.l1)
  } > aicore{{ aicore }}_l1

  .aicore{{ aicore }}.ddr : ALIGN(64) {
    *(.aicore{{ aicore }}.ddr)
  } > dram

  /* magic memory for UVM SW IPC communication */
  PROVIDE(aicore{{ aicore }}__uvm_sw_ipc_mem = UVM_SW_IPC_BASE);
#endif

{% endfor %}

{% for pve in pves %}
  /* ==== */
  /* PVE{{ pve }} */
  /* ==== */
#if PROCESSOR_PVE{{ pve }}
  .pve{{ pve }}.text.init : ALIGN(64) {
    KEEP(*(.pve{{ pve }}.text.init))
  } > pve{{ pve }}_spm
  ASSERT(
    ADDR(.pve{{ pve }}.text.init) == ORIGIN(pve{{ pve }}_spm),
    "Error: PVE{{ pve }} boot code not at start of PVE{{ pve }} SPM"
  )
  .pve{{ pve }}.text : ALIGN(64) {
    *(.pve{{ pve }}.text .pve{{ pve }}.text.*)
  } > pve{{ pve }}_spm
  .pve{{ pve }}.data : ALIGN(64) {
    *(.pve{{ pve }}.rodata .pve{{ pve }}.rodata.*)
    *(.pve{{ pve }}.data .pve{{ pve }}.data.*)
  } > pve{{ pve }}_spm
  .pve{{ pve }}.sdata : ALIGN(64) {
    pve{{ pve }}___global_pointer$ = . + 0x800;
    *(.pve{{ pve }}.srodata .pve{{ pve }}.srodata.*)
    *(.pve{{ pve }}.sdata .pve{{ pve }}.sdata.*)
  } > pve{{ pve }}_spm
  .pve{{ pve }}.sbss : ALIGN(64) {
    pve{{ pve }}__bss_begin = .;
    *(.pve{{ pve }}.sbss .pve{{ pve }}.sbss.*)
  } > pve{{ pve }}_spm
  .pve{{ pve }}.bss : ALIGN(64) {
    *(.pve{{ pve }}.bss .pve{{ pve }}.bss.*)
    pve{{ pve }}__bss_end = .;
  } > pve{{ pve }}_spm

#ifndef SYSTEM_TOP
  /* TLS for PVE {{ pve }} (standalone) */
  .pve{{ pve }}.tdata : ALIGN(64) {
    pve{{ pve }}__tdata_begin = .;
    *(.pve{{ pve }}.tdata .pve{{ pve }}.tdata.*)
    . = ALIGN(64);
    pve{{ pve }}__tdata_end = .;
    pve{{ pve }}__tbss_begin = .;
    *(.pve{{ pve }}.tbss .pve{{ pve }}.tbss.*)
    . = ALIGN(64);
    pve{{ pve }}__tbss_end = .;
  } > pve{{ pve }}_spm
  pve{{ pve }}__tls_alloc_size = pve{{ pve }}__tbss_end - pve{{ pve }}__tdata_begin;
#endif

  .pve{{ pve }}.runtime.tls : ALIGN(64) {
    pve{{ pve }}__tls_begin = .;
    . = . + 8 * pve{{ pve }}__tls_alloc_size;
    pve{{ pve }}__tls_end = .;
  } > pve{{ pve }}_spm
  .pve{{ pve }}.runtime.stack : ALIGN(64) {
    pve{{ pve }}__stack_begin = .;
    . = . + 8 * PVE_STACK_SIZE;
    pve{{ pve }}__stack_end = .;
  } > pve{{ pve }}_spm

  .pve{{ pve }}.l1 : ALIGN(64) {
    *(.pve{{ pve }}.l1)
  } > pve{{ pve }}_l1_0

  .pve{{ pve }}.ddr : ALIGN(64) {
    *(.pve{{ pve }}.ddr)
  } > dram

  /* magic memory for UVM SW IPC communication */
  PROVIDE(pve{{ pve }}__uvm_sw_ipc_mem = UVM_SW_IPC_BASE);
#endif
{% endfor %}


  /* discard everything else */
  /DISCARD/ : {
    *(.text .text.*)
    *(.rodata .rodata.*)
    *(.data .data.*)
    *(.bss .bss.*)
    *(.sdata .sdata.*)
    *(.sbss .sbss.*)
    *(.scommon .scommon.*)
    *(.tdata .tdata.*)
    *(.tbss .tbss.*)
    *(.ddr .ddr.*)
    *(.l2 .l2.*)
    *(.aicore0.*)
    *(.pve0.*)
    *(.tohost)
  }
}

